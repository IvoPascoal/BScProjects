{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning / Aprendizagem Automática\n",
    "\n",
    "## Sara C. Madeira, 2019/20\n",
    "\n",
    "# Practical 02 + Practical 03 - Introduction to Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Getting Started\n",
    "\n",
    "In this tutorial we use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org), [Scikit-learn](http://scikit-learn.org/stable/) and other Python technical libraries such as [Pandas](http://pandas.pydata.org). \n",
    "\n",
    "**In the lab** Python 3, Jupyter and Scikit-learn should be installed in both Windows and Linux. \n",
    "\n",
    "**At home** you can get Python 3, Jupyter and Scikit-learn by installing [Anaconda](https://www.anaconda.com). \n",
    "\n",
    "Once you have Python 3, Jupyter and Scikit-learn you can follow this tutorial by running the examples interactively using the Jupyter notebook (file AA_201920_TP02_TP03.ipynb) \n",
    "or running the examples using an IDE such as Spyder that is installed with Anaconda.\n",
    "\n",
    "The tutorial has two main parts:\n",
    "\n",
    "* **PART 1. Python Technical Libraries**: provinding an overview on Pandas, NumPy, Scikit-learn, Matplotlib and SciPy. The focus is on Pandas, a powerful tool for data analysis, and on the relationship between Pandas, NumPy, and Scikit-learn. \n",
    "\n",
    "You should complete this part in **Practical 02**.\n",
    "\n",
    "* **PART 2. Scikit-learn**: providing a first introduction to Scikit-learn by using a classical supervised learning problem as example. \n",
    "\n",
    "You should complete this part in **Practical 03**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Python's Technical Libraries (Practical 02)\n",
    "\n",
    "Python has an excellent suite of libraries available that make it a first‐class environment for technical computing. The main ones are the following ([The Data Science Handbook, F. Cady, Wiley, 2017](http://eu.wiley.com/WileyCDA/WileyTitle/productCd-1119092949.html)):\n",
    "* **Pandas**: This is a very important library for data analysis. It stores and operates on data in data frames, very efficiently and with a sleek, intuitive API. More info on Pandas [here](http://pandas.pydata.org). \n",
    "* **NumPy**: A library for dealing with numerical arrays in ways that are fast and memory efficient, but it is clunky and low level for a user. Under the hood, Pandas operates on NumPy arrays. More info on NumPy [here](http://www.numpy.org).\n",
    "* **Scikit‐learn**: This is the main machine learning library and it operates on NumPy arrays. You can take Pandas objects, turn them into NumPy arrays, and then plug them into scikit‐learn. More info on Scykit-learn [here](http://scikit-learn.org/stable/).\n",
    "* **Matplotlib**: This is the main plotting and visualization library. Similar to NumPy, it is low level and a bit clunky to use directly. Pandas provides human‐friendly wrappers that call matplotlib routines. More info on Matplotlib [here](https://matplotlib.org)\n",
    "* **SciPy**: This provides a suite of functions that perform fancy numerical operations on NumPy arrays. More info on SciPy [here](https://www.scipy.org)\n",
    "\n",
    "Note that these are not the only technical computing libraries available in Python, but they are by far the most popular, and together they form a cohesive, powerful tool suite for data analysis and machine learning. **NumPy** is the most fundamental library; it defines the core numerical arrays that everything else operates on. However, most of your actual code (especially data munging and feature extraction) will be working within **Pandas**, only switching to the other libraries as needed, and the machine learning algorithms will be executed using **Scikit‐learn**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. NumPy\n",
    "\n",
    "NumPy is one of the fundamental packages for scientific computing in Python. It contains functionalities for multidimensional arrays, high-level mathematical functions such as linear algebra operations and the Fourier transform and pseudo random number generators.\n",
    "\n",
    "**The NumPy array is the fundamental data structure in scikit-learn**. Scikit-learn takes in data in the form of NumPy arrays. Any data you are using will have to be converted to a NumPy array. \n",
    "\n",
    "The core functionality of NumPy is this **``ndarray``** meaning it has n dimensions and all elements of the array must be the same type. \n",
    "\n",
    "A NumPy array looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a ndarray\n",
    "\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ndarray** can be used like a matrix with index access to rows and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row\n",
    "\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value on first row and second column\n",
    "\n",
    "x[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both Pandas and Scikit-learn use NumPy functions and the ndarray data structure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. SciPy\n",
    "\n",
    "SciPy is a collection of functions for scientific computing. It provides, among other functionality, advanced linear algebra routines, mathematical function optimization, signal processing, special mathematical functions, and statistical distributions. **Scikit-learn draws from SciPy’s collection of functions for implementing its algorithms.**\n",
    "\n",
    "The most important part of SciPy for us is scipy.sparse with provides **sparse matrices**, which is **another representation that is used for data in Scikit-learn**. Sparse matrices are used whenever we want to store a 2D-array that contains mostly zeros and we want to efficiently store all the non-zero values without wasting memory storing the zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2d numpy array with a diagonal of ones, and zeros everywhere else\n",
    "\n",
    "eye = np.eye(4)\n",
    "\n",
    "print(\"Numpy array:\\n\", eye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy array to a scipy sparse matrix in CSR format\n",
    "# In CSR (Compressed Sparse Row matrix) format only the non-zero entries are stored\n",
    "\n",
    "sparse_matrix = sparse.csr_matrix(eye)\n",
    "\n",
    "print(\"\\nScipy sparse CSR matrix:\\n\", sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Matplotlib\n",
    "\n",
    "Matplotlib is the primary scientific plotting library in Python. It provides functions for making publication-quality visualizations such as line charts, histograms, scatter plots, and so on. Visualizing your data and any aspects of your analysis can give you important insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the 'inline' backend only in Jupyter\n",
    "# This allows the matplotlib graphs to be included in the notebook next to the code\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Generate a sequence of integers\n",
    "x = np.arange(20)\n",
    "\n",
    "# create a second array using sinus\n",
    "y = np.sin(x)\n",
    "\n",
    "# The plot function makes a line chart of one array against another\n",
    "plt.plot(x, y, marker=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Pandas\n",
    "\n",
    "[Pandas](http://pandas.pydata.org) is a powerful and widely used Python library for data wrangling and analysis. It is built around a data structure called **DataFrame**, that is modeled after the R DataFrame. Simply put, a Pandas DataFrame is a table, similar to an Excel Spreadsheet. Pandas provides a great range of methods to modify and operate on this table, in particular it allows SQL-like queries and joins of tables. Another valuable tool provided by Pandas is its ability to ingest from a great variety of file formats and databases, such as SQL, Excel files, and comma separated value (CSV) files.\n",
    "\n",
    "The rest of this section will be a quick crash course on the **basic data structures of Pandas: Data Frames and Series** following ideas and examples in [The Data Science Handbook, F. Cady, Wiley, 2017](http://eu.wiley.com/WileyCDA/WileyTitle/productCd-1119092949.html). You can read more on Pandas [here](http://pandas.pydata.org). A very good reference on Pandas is the book ([Python for Data Analysis](http://wesmckinney.com/pages/book.html) written by its author Wes Mckinney."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frames\n",
    "\n",
    "The central data structure in Pandas is called a **DataFrame**, which is a table with rows and columns, where each column holds data of a particular type (such as integers, strings, or floats). \n",
    "\n",
    "DataFrames make it easy and efficient to apply a function to every element in a column or to calculate aggregates such as the sum of a column. Some of the basic operations on data frames are shown is what follow (**more info** [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pandas and alias it as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a DataFrame from a Dictionary that maps column names to their values\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"name\": [\"Bob\", \"Alex\", \"Janice\"],\n",
    "  \"age\": [60, 25, 33]\n",
    "  })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size\n",
    "\n",
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of columns \n",
    "\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column identifiers\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows \n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking values of a specific column\n",
    "\n",
    "df[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking values of a specific column\n",
    "\n",
    "df[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making new columns from old ones is really easy\n",
    "\n",
    "df[\"age_plus_one\"] = df[\"age\"] + 1 \n",
    "\n",
    "df[\"age_times_two\"] = 2 * df[\"age\"] \n",
    "\n",
    "df[\"age_squared\"] = df[\"age\"]**2\n",
    "\n",
    "df[\"over_30\"] = (df[\"age\"] > 30) # this column is bools\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking values of column \"over_30\"\n",
    "\n",
    "df[\"over_30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns can have several built-in aggregate functions such as:\n",
    "\n",
    "#sum\n",
    "total_age = df[\"age\"].sum()\n",
    "\n",
    "total_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns can have several built-in aggregate functions such as:\n",
    "\n",
    "# median\n",
    "median_age = df[\"age\"].quantile(0.5)\n",
    "\n",
    "median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns can have several built-in aggregate functions such as:\n",
    "\n",
    "# mean\n",
    "mean_age = df[\"age\"].mean()\n",
    "\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can select several rows of the DataFrame and make a new DataFrame out of them\n",
    "\n",
    "print(df)\n",
    "\n",
    "df_age_below_50 = df[df[\"age\"] < 50]\n",
    "\n",
    "df_age_below_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can select several rows of the DataFrame and make a new DataFrame out of them\n",
    "\n",
    "df_below_30 = df[df[\"over_30\"] == False]\n",
    "\n",
    "df_below_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also make selections using more complex logic expressions\n",
    "\n",
    "# all rows with 'age' above 30, but without the name  \"Bob\"\n",
    "\n",
    "df_30_notBob = df[(df['age']>30) & ~(df['name'] == 'Bob')]\n",
    "df_30_notBob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also apply a custom function to a column \n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "df[\"age_squared\"] = df[\"age\"].apply(f)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a custom function to a column - another way\n",
    "\n",
    "df[\"age_squared\"] = df[\"age\"].apply(lambda x: x**2)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the index\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"name\": [\"Bob\", \"Alex\", \"Jane\"],\n",
    "  \"age\": [60, 25, 33]\n",
    "  })\n",
    "\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints 0‐2, the line numbers\n",
    "\n",
    "nr = len(df) # number of rows\n",
    "\n",
    "for i in range(nr):\n",
    "    print(df.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get row 0 (data in df is indexed by row number starting at 0)\n",
    "\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing the same data, but where column \"name\" is the index\n",
    "\n",
    "df_w_name_as_index = df.set_index(\"name\")\n",
    "\n",
    "df_w_name_as_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in df is now indexed by name\n",
    "\n",
    "df_w_name_as_index.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the row for Bob (row where \"name\"='Bob')\n",
    "\n",
    "bobs_row = df_w_name_as_index.loc[\"Bob\"] \n",
    "\n",
    "bobs_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value in column \"age\" in the row where \"name\" is Bob\n",
    "\n",
    "bobs_row[\"age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Modify the following code to create a DataFrame from Dictionary data and then try some of the functions described [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple dataset of people\n",
    "\n",
    "data = {'Name': [\"John\", \"Anna\", \"Peter\", \"Linda\"],\n",
    "        'Location' : [\"New York\", \"Paris\", \"Berlin\", \"London\"],\n",
    "        'Age' : [24, 13, 53, 33]\n",
    "       }\n",
    "data\n",
    "\n",
    "# create a DataFrame from the dictionary data\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Create a **.csv** file (comma separated file) named **``myfile.csv``** with 4 rows and 2 columns. The first row should have the column names: **``name``** and **``age``**, and rows 1 to 3 the following values:  Bob, 60; Alex, 25; and Jane, 33. \n",
    "\n",
    "Create a Data Frame from your .csv file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a DataFrame from a file\n",
    "\n",
    "other_df = pd.read_csv(\"myfile.csv\")\n",
    "\n",
    "other_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "Besides DataFrames, the other important data structure in Pandas is **Series**: a column in a DataFrame is a Series. Conceptually, a Series is just an array of data objects with an index associated. The columns of a DataFrame are Series objects that all happen to share the same index. The following code shows some  basic Series operations (**more info** [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pandas and alias it as pd\n",
    "import pandas as pd\n",
    "\n",
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Series from list\n",
    "\n",
    "s = pd.Series([1,2,3]) \n",
    "\n",
    "# display the values in s. Note that the index is on the left.\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a number to each element of s\n",
    "\n",
    "s+2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that s did not change\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want the change to be efective\n",
    "\n",
    "s = s+2\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding two series will add corresponding elements to each other\n",
    "\n",
    "s + pd.Series([4,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note again that s did not change\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that as above now s changes\n",
    "\n",
    "s = s + pd.Series([4,4,5])\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining and Grouping\n",
    "\n",
    "So far we have focused on the following DataFrame operations: \n",
    "* Creating data frames\n",
    "* Adding new columns that are derived from basic operations on existing columns\n",
    "* Using simple conditions to select rows in a DataFrame\n",
    "* Aggregating columns\n",
    "* Setting columns to function as an index, and using the index to pull out rows of the data.\n",
    "\n",
    "This section discusses two more advanced operations: **joining and grouping**. \n",
    "\n",
    "These may be familiar to you from working with SQL. You can read more on **join** [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) and more on **groupby** [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html).\n",
    "\n",
    "[Joining](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) is used if you want to combine two separate data frames into a single frame containing all the data. \n",
    "\n",
    "We take two data frames, match up rows that have a common index, and combine them into a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_age = pd.DataFrame({\n",
    "  \"name\": [\"Tom\", \"Tyrell\", \"Claire\"],\n",
    "  \"age\": [60, 25, 33]\n",
    "  })\n",
    "\n",
    "df_w_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_height = pd.DataFrame({\n",
    "  \"name\": [\"Tom\", \"Tyrell\", \"Claire\"],\n",
    "  \"height\": [6.2, 4.0, 5.5]\n",
    "  })\n",
    "\n",
    "df_w_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df_w_age and df_w_height using the common index \"name\"\n",
    "# (Index should be similar to one of the columns)\n",
    "\n",
    "joined = df_w_age.set_index(\"name\").join(df_w_height.set_index(\"name\"))\n",
    "\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then create an index on row number\n",
    "\n",
    "joined.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Grouping](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) is used when we want to group the rows based on some property and aggregate each group separately. \n",
    "\n",
    "This is done with the **groupby** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a Dictionary\n",
    "\n",
    "df = pd.DataFrame({\"name\": [\"Tom\", \"Tyrell\", \"Claire\"], \n",
    "                    \"age\": [60, 25, 33],\n",
    "                   \"height\": [6.2, 4.0, 5.5],\n",
    "                   \"gender\": [\"M\", \"M\", \"F\"]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use built-in aggregation functions: groupby gender and compute mean\n",
    "\n",
    "means = df.groupby(\"gender\").mean()\n",
    "\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use built-in aggregation functions: groupby gender and compute median\n",
    "\n",
    "medians = df.groupby(\"gender\").quantile(0.5)\n",
    "\n",
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append two extra rows to df\n",
    "\n",
    "df_new = pd.DataFrame({\"name\": [\"Peter\", \"Susan\"], \n",
    "                    \"age\": [60, 25],\n",
    "                   \"height\": [6.2,  5.5],\n",
    "                   \"gender\": [\"M\", \"F\"]})\n",
    "\n",
    "df = df.append(df_new)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby age and computer mean\n",
    "\n",
    "mean = df.groupby(\"age\").mean()\n",
    "\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a custom aggregation function\n",
    "\n",
    "def agg(df):\n",
    "    return pd.Series({\"name\": max(df[\"name\"]), \n",
    "                  \"oldest\": max(df[\"age\"]),\n",
    "                  \"mean_height\": df[\"height\"].mean()})\n",
    "\n",
    "# groupby gender and apply function agg\n",
    "\n",
    "df.groupby(\"gender\").apply(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a DataFrame to a NumPy array\n",
    "\n",
    "**To convert a DataFrame into a NumPy array use the .values property. This is important since Scikit-learn works with NumPy arrays.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x': [1, 2, 3, 4, 5],\n",
    "                     'y': [0.01, -0.01, 0.25, -4.1, 0.],\n",
    "                     'z': [-1.5, 0., 3.6, 1.3, -2.]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfValuesAsArray = df.values\n",
    "\n",
    "type(dfValuesAsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfValuesAsArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some machine learning models, you may only wish to **use a subset of the columns**. \n",
    "\n",
    "In this case you can use **loc indexing with values** to get the desired subset of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = ['x', 'y']\n",
    "\n",
    "df.loc[:, model_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To convert back to a DataFrame you can pass a two-dimensional ndarray with optional column names:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(dfValuesAsArray)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(dfValuesAsArray, columns=['x', 'y', 'z'])\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the .values attribute contains only numeric types as in the example above, when your data is homogeneous, but the **.values attribute can also be an ndarray of Python objects**, when your data is heterogeneous as in the example below. \n",
    "\n",
    "In either case **you can use the ndarray obtained from .values in Skikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'x': [1, 2, 3, 4, 5],\n",
    "                     'y': [0.01, -0.01, 0.25, -4.1, 0.],\n",
    "                     'z': [-1.5, 0., 3.6, 1.3, -2.]})\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column named \"strings\" to the DataFrame\n",
    "\n",
    "df3['strings'] = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df3.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3AsArray = df3.values\n",
    "\n",
    "df3AsArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scikit-learn (Practical 03)\n",
    "\n",
    "This section is based almost entirely on Chapter 1 from the book [Introduction to Machine Learning with Python: A Guide for Data Scientists, Sarah Guido&Andreas Müller, 2016](https://www.safaribooksonline.com/library/view/introduction-to-machine/9781449369880/).  We will go through a simple machine learning application and create our first model.\n",
    "\n",
    "### A First Application: Classifying iris species\n",
    "\n",
    "Let’s assume that a hobby botanist is interested in distinguishing what is the species of some iris flowers that she found. She has collected some measurements associated with the iris: the length and width of the petals, and the length and width of the sepal, all measured in centimeters.\n",
    "\n",
    "She also has the measurements of some irises that have been previously identified by an expert botanist as belonging to the species Setosa, Versicolor or Virginica. For these measurements, she can be certain of which species each iris belongs to. Let’s assume that these are the only species our hobby botanist will encounter in the wild.\n",
    "\n",
    "**Our goal is to build a machine learning model that can learn from the measurements of these irises whose species is known, so that we can predict the species for a new iris.** Thus, the desired output for a single data point (an iris) is the predicted species of this flower. This is a classical **supervised learning problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Meet the Data\n",
    "\n",
    "We use the **iris dataset**, a classical dataset in machine learning and statistics. It is included in scikit-learn in the **datasets module** and can be loaded by calling the load_iris function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris object that is returned by load_iris is a **Bunch object**, which is very similar to a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to a Dictionary **it contains keys and values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can see a description of the dataset by printing the DESCR key:\n",
    "\n",
    "print(iris_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value with key target_names is an array of strings, containing the species of flower we want to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values in key target_names\n",
    "\n",
    "iris_dataset['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data itself is contained in the target and data fields.**\n",
    "\n",
    "The data contains the numeric measurements of sepal length, sepal width, petal length, and petal width in a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values in key data\n",
    "\n",
    "iris_dataset['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris_dataset['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows in the data array correspond to flowers, while the columns represent the four measurements that were taken for each flower. \n",
    "\n",
    "Take a look at the ** size of the dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The shape of the data array is the number of samples times the number of features.\n",
    "This is a convention in scikit-learn and your data will always be assumed to be in this shape.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature values of the first 5 examples\n",
    "\n",
    "iris_dataset['data'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target array contains the species of each of the flowers that were measured, also\n",
    "as a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "\n",
    "type(iris_dataset['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target of the first 5 learning examples\n",
    "\n",
    "iris_dataset['target'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The target is a one-dimensional array**, with one entry per flower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The species are encoded as integers from 0 to 2. The meaning of the numbers are given by the iris['target_names'] array: 0 means Setosa, 1 means Versicolor and 2 means Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "\n",
    "iris_dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Measuring Success: Training and Testing data\n",
    "\n",
    "To assess the models’ performance, we show the model new data (that it hasn’t seen before) for which we have labels. This is usually done by splitting the labeled data we have collected (here our 150 flower measurements) into two parts.\n",
    "One part of the data is used to build our machine learning model and is called the **training data or training set**. The rest of the data will be used to access how well the model works and is called test data, **test set or hold-out set**.\n",
    "\n",
    "**Scikit-learn contains a function that shuffles the dataset and splits it for you, the train_test_split function.** This function extracts 75% of the rows in the data as the training set, together with the corresponding labels for this data. The remaining 25% of the data, together with the remaining labels are declared as the test set. How much data you want to put into the training and the test set respectively depends on the problem, but using a test-set containing 25% of the data is a good rule of thumb.\n",
    "\n",
    "**In scikit-learn, data is usually denoted with a capital X, while labels are denoted by a lower-case y.**\n",
    "\n",
    "Let's call train_test_split on our data and assign the outputs using this nomenclature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], \n",
    "                                                    iris_dataset['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that we will get the same output if we run the same function several times, we provide the pseudo random number generator with a fixed seed using the **random_state parameter**. This will make the outcome deterministic, so this line will always have the same outcome.\n",
    "\n",
    "The train_test_split function outputs X_train, X_test, y_train and y_test, which are all numpy arrays. X_train contains 75% of the rows of the dataset, and X_test contains the remaining 25%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. First Things  First: Look at Your Data\n",
    "\n",
    "Before building a machine learning model, it is often a good idea to inspect the data, to see if the task is easily solvable without machine learning, or if the desired information might not be contained in the data. One of the best ways to inspect data is to visualize it. One way to do this is by using a **scatter plot**. \n",
    "\n",
    "A scatter plot of the data puts one feature along the x-axis, one feature along the y- axis, and draws a dot for each data point. Unfortunately, computer screens have only two dimensions, which allows us to only plot two (or maybe three) features at a time. It is difficult to plot datasets with more than three features this way. One way around this problem is to **do a pair plot, which looks at all pairs of two features**. If you have a small number of features, such as the four we have here, this is quite reasonable. You should keep in mind that a pair plot does not show the interaction of all of features at once, so some interesting aspects of the data may not be revealed when visualizing it this way.\n",
    "\n",
    "Here is a pair plot of the features in the training set (don't worry abour the details in the code, the important here are the plots). The data points are colored according to the species the iris belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
    "\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "\n",
    "pd.plotting.scatter_matrix(iris_dataframe, \n",
    "                           c=y_train, \n",
    "                           figsize=(15, 15),\n",
    "                           marker='o', \n",
    "                           hist_kwds={'bins': 20}, \n",
    "                           s=60, \n",
    "                           alpha=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we can see that the three classes seem to be relatively well separated using the sepal and petal measurements. This means that a simple machine learning model will likely be able to learn to separate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Building your First model: k-Nearest Neighbors\n",
    "\n",
    "Now we can start building the actual machine learning model. There are many classification algorithms in scikit-learn that we could use. Here we will use a **k-nearest neighbors classifier**, which is easy to understand: this algorithm stores the training set, and in order to make a prediction for a new data point finds the point in the training set that is closest to the new point. Then, it simply assigns the label of this closest data training point to the new data point.\n",
    "\n",
    "The k in k nearest neighbors stands for the fact that instead of using only the closest neighbor to the new data point, we can consider any fixed number k of neighbors in the training (for example, the closest three or five neighbors). Then, we can make a prediction using the majority class among these neighbors. We use only a single neighbor for now.\n",
    "\n",
    "**All machine learning models in scikit-learn are implemented in their own class, which are called Estimator classes.**\n",
    "\n",
    "The k nearest neighbors classification algorithm is implemented in the **KNeighborsClassifier** class in the neighbors module. Before we can use the model, we need to instantiate the class into an object. This is when we will set any parameters of the model. The single parameter of the KNeighbor sClassifier is the number of neighbors, which we will set to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# use k nearest neigbors with k=1\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knn object encapsulates the algorithm to build the model from the training data, as well the algorithm to make predictions on new data points. It will also hold the information the algorithm has extracted from the training data. \n",
    "\n",
    "**To build the model on the training set, we call the fit method of the knn object**, which takes as arguments the numpy array X_train containing the training data and the NumPy array y_train of the corresponding training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Making Predictions\n",
    "\n",
    "**We can now use the trained model to make predictions on new data, for which we might not know the correct labels.** \n",
    "\n",
    "Imagine we found an iris in the wild with a sepal length of 5cm, a sepal width of 2.9cm, a petal length of 1cm and a petal width of 0.2cm. What species of iris would this be ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the new data in a NumPy array\n",
    "\n",
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To make prediction we call the predict method** of the knn object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = knn.predict(X_new)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicts that this new iris belongs to the class 0, meaning its species is Setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset['target_names'][prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens if we use 3 neighbors ?\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn3.fit(X_train, y_train)\n",
    "\n",
    "prediction2 = knn3.predict(X_new)\n",
    "\n",
    "iris_dataset['target_names'][prediction2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The prediction does not change, although it could change since the model is not the same.**\n",
    "\n",
    "**How do we know whether we can trust the model?** We don’t know the correct species of this sample, and we shoudn't know since this is the whole point of building the model, but can we somehow evaluate how good is the model, that is, how likely is how model to behave well in unseen data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Evaluating the Model\n",
    "\n",
    "**This is where the test set that we created earlier comes in.** \n",
    "\n",
    "This data was not used to build the model, but we do know what the correct species are for each iris in the test set.\n",
    "\n",
    "In this context, we can make a prediction for an iris in the test data, and compare it against its label (the known species). \n",
    "\n",
    "We can then measure how well the model works by computing, for example, its **accuracy**, which is the fraction of flowers for which the right species was predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of knn with 1 neigbour\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the score method of the knn object, which will compute the **accuracy in the test set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, the test set accuracy is about 0.97, which means we made the right prediction for 97% of the irises in the test set. \n",
    "\n",
    "Under some mathematical assumptions, this means that we can expect our model to be correct 97% of the time for new irises. Thus, for our hobby botanist application, this a high level of accuracy means that our models may be trustworthy enough to use.\n",
    "\n",
    "Note that in this case the accuracy in the train set is 100%. **Note also that the accuracy  in the train set is usually highter than the accuracy in the test set, although not 100% in most cases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_train)\n",
    "\n",
    "np.mean(y_pred == y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Summary\n",
    "   \n",
    "**The following code contains the core code for applying any machine learning algorithms using scikit-learn. The fit, predict and score methods are the common interface to supervised models in scikit-learn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], \n",
    "                                                     iris_dataset['target'],\n",
    "                                                    random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once we have trained the model (classifier) we can use it classify new examples using the function predict**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "\n",
    "prediction = knn.predict(X_new)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classes and Functions in Scikit-learn\n",
    "\n",
    "[Scikit-learn organization](http://scikit-learn.org/stable/) is as follows:\n",
    "\n",
    "* [Classification](http://scikitlearn.org/stable/supervised_learning.html#supervised-learning)\n",
    "\n",
    "* [Regression](http://scikitlearn.org/stable/supervised_learning.html#supervised-learning)\n",
    "\n",
    "* [Clustering](http://scikit-learn.org/stable/modules/clustering.html#clustering)\n",
    "\n",
    "* [Dimensionality reduction](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)\n",
    "\n",
    "* [Model selection](http://scikit-learn.org/stable/model_selection.html#model-selection)\n",
    "\n",
    "* [Preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)\n",
    "\n",
    "Classification and regression enable [Supervised Learning](http://scikitlearn.org/stable/supervised_learning.html#supervised-learning), while clustering enables [Unsupervised Learning](http://scikit-learn.org/stable/modules/clustering.html#clustering). \n",
    "\n",
    "**Class and function reference of scikit-learn is available in the [API Reference](http://scikit-learn.org/stable/modules/classes.html). \n",
    "\n",
    "For further details refer to the [full user guide](http://scikit-learn.org/stable/modules/classes.html).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
