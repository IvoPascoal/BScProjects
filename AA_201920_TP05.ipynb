{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning / Aprendizagem Automática\n",
    "\n",
    "## Sara C. Madeira, 2019/20\n",
    "\n",
    "# Practical 05 - Decision Trees in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Requirements\n",
    "\n",
    "This practical studies decision trees with [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org), [Scikit-learn](http://scikit-learn.org/stable/), as well as other Python technical libraries, such as [Pandas](http://pandas.pydata.org) and [NumPy](http://www.numpy.org).\n",
    "\n",
    "**Decision trees can handle both categorical and numerical features in the same dataset. This is actualy one of their strenghts together with interpretability.** Some decision tree learning algorithms, such C4.5 can deal with categorical features natively, thus being transparent to the user how the different attribute types are managed by the algorithm, while others, such as CART, require data transformations to deal with categorical features. Despite the algorithm, also its implementation may force the user to data transformations before the algorithm can run and/or be used correctely. **In Scikit-learn, for example, all learning algorithms are implemented to receive as input numeric values, preventing the use of certain types of features, such as categorical, without data transformations.**\n",
    "\n",
    "Last week we used `Orange3` and its implementation of **C4.5 algorithm** (Quinlan, 1993) to learn decision trees. C4.5 uses the **Information Gain** as attribute selection measure (impurity measure), nativelly suports any type of feature (numeric and categorical atributes), and it can learn **decision trees that might not be binary**. C4.5 allows binary and multiclass classification but does not work with numeric class, that is, it cannot be used to learn a regression tree. **Orange3 implementations of C4.5 handles correctely and natively any feature type.**\n",
    "\n",
    "**Scikit-learn uses an optimised version of the CART algorithm** (Breiman, Friedman, Olshen, Stone, 1986). CART (Classification and Regression Trees) uses the **Gini Index** as attribute selection measure; allows binary and multiclass classification together with regression; and learns **binary trees**. \n",
    "\n",
    "**In order to learn decision trees or any other classifier from non-numerical data, Scikit-learn requires feature transformations, aka encoding.**\n",
    "\n",
    "This tutorial has two main parts and a third that summarizes the topic of **learning decision trees in Scikit-Learn**:\n",
    "\n",
    "1. Learning Decision Tree from **Numerical Features**\n",
    "\n",
    "2. Learning Decision Tree from **Categorical Features**\n",
    "\n",
    "3. Learning Decision Tree from **Multiple Types of Features**\n",
    "\n",
    "\n",
    "### Decision Trees and Encoding\n",
    "\n",
    "Summing up, when learning decision tree models from categorical features, we might come across three types of algorithms/implementations:\n",
    "\n",
    "1. **Algorithms handling categorical features CORRECTLY**. We input the categorical features to the algorithm in the appropriate format, as we do with the numeric features (since we can have features of any type), and the machine learning algorithm processes categorical features correctly as categorical. This is the BEST CASE since it fits our needs and we do not have to worry about feature transformations. This is the case of Orange3, and it is also the case of Weka and other machine learning tools.\n",
    "\n",
    "2. **Models handling categorical features INCORRECTLY**. We input the categorical features to the algorithm in the appropriate format, BUT the machine learning algorithm processes categorical features incorrectly by doing wizardry processing to transform them into something usable. This is the WORST CASE EVER since it probabbly does not do what we expect to be done, and thus features are wrongly transformation, and consequentely the model performance will be compromised.\n",
    "\n",
    "3. **Models NOT handling categorical features at all**. In this case we have to preprocess manually the categorical features in order to have them in an appropriate format for the machine learning algorithm, usually numeric features. **This is the case in Scikit-Learn, where we have to transform (aka ENCODE) the categorical features before learning the decision tree.** But how do we transform (aka ENCODE) them? There are many methods to encode categorical features. We are going to explore the use of three of them: **Binary Encoding**, **One-Hot encoding (Dummy Variables)**, and **Numeric Encoding**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Decision Trees from Numeric Features\n",
    "\n",
    "### 1.1. Getting Started: Learning a Decision Tree using All Data\n",
    "\n",
    "We introduce decidion trees in Scikit-learn using datasets where the features are numeric. We first use the well-known [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). \n",
    "\n",
    "As you might remember the goal is to distinguish the species of iris flowers given that they are characterised by the length and width of the petals, and the length and width of the sepal, all measured in centimeters. \n",
    "\n",
    "In this context, we have at hands a **multi-class classification problem, where the class has 3 possible values (Setosa, Versicolor, or Virginica) and the iris examples are characterized by 4 numeric features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general view of the dataset\n",
    "\n",
    "iris_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values in key target_names\n",
    "\n",
    "iris_dataset['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset\n",
    "\n",
    "# The shape of the data array is the number of examples times the number of features.\n",
    "# This is a convention in scikit-learn and your data will always be assumed to be in this shape.\n",
    "\n",
    "iris_dataset['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature values of the first 5 examples\n",
    "\n",
    "iris_dataset['data'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target of the first 5 learning examples\n",
    "\n",
    "# (Setosa, Versicolor, Virginica) are coded as (0, 1, 2)\n",
    "\n",
    "iris_dataset['target'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target is always a one-dimensional array, with one entry per example\n",
    "\n",
    "iris_dataset['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The species are encoded as integers from 0 to 2. \n",
    "# The meaning of the numbers are given by the iris['target_names'] array: \n",
    "# 0 means Setosa\n",
    "# 1 means Versicolor\n",
    "# 2 means Virginica\n",
    "\n",
    "iris_dataset['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now learn the **decision tree classifier** (http://scikit-learn.org/stable/modules/tree.html) using the class `DecisionTreeClassifier` (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). \n",
    "\n",
    "**The parameter `criterion` defines the function to measure the quality of a split. Supported criteria are “gini” for the Gini Index and “entropy” for the Information Gain. Gini is the `default`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#Learning a decision tree using CART and the Gini index as impurity criteria (default)\n",
    "\n",
    "dtc_Gini = tree.DecisionTreeClassifier() # criterion = \"Gini\"\n",
    "\n",
    "dtc_Gini = dtc_Gini.fit(iris_dataset.data, iris_dataset.target)\n",
    "\n",
    "dtc_Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn allows to export the decision tree as a .dot file after training**, which we can visualize using **`GraphViz`**. `GraphViz` is freely available at http://www.graphviz.org and it is supported by Linux, Windows, and Mac OS X. \n",
    "\n",
    "**If `GraphViz` is not installed in your computer you should install it. If you are not able to install it now don't worry, there is an alternative below.**\n",
    "\n",
    "In this context, we first **create the .dot file via scikit-learn using the `export_graphviz` function from the `tree` submodule**, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(dtc_Gini, out_file=\"iris_Gini.dot\",\n",
    "                                feature_names=iris_dataset.feature_names,\n",
    "                                class_names=iris_dataset.target_names,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing GraphViz, we can **convert the tree.dot file into a PNG file** by executing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['dot', '-T', 'png', 'iris_Gini.dot', '-o', 'iris_Gini.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to your working folder and **open the file `'iris_Gini.png'`**. \n",
    "\n",
    "As you can see **the decision tree learned using the Gini Index is the following:**\n",
    "\n",
    "<img src=\"iris_Gini.png\" alt=\"iris_Gini.png\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In case you are not allowed or are not able to install Graphviz**, you can **vizualise the `.dot` file online** by using for instance [`GraphvizOnline`](https://dreampuf.github.io/GraphvizOnline/).\n",
    "\n",
    "You should first copy/paste the `iris_Gini.dot` file contents to the editor area (left).\n",
    "\n",
    "<img src=\"graphviz_online.png\" alt=\"graphviz_online.png\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now check if we obtain the same decision tree if we use Information Gain instead of Gini Index as impurity measure:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#Learning a decision tree using CART and the Information Gain as impurity criteria \n",
    "\n",
    "dtc_IG = tree.DecisionTreeClassifier(criterion = \"entropy\") # criterion = \"entropy\"\n",
    "dtc_IG = dtc_IG.fit(iris_dataset.data, iris_dataset.target)\n",
    "dtc_IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a .dot file with the tree\n",
    "dot_data = tree.export_graphviz(dtc_IG, out_file=\"iris_IG.dot\",\n",
    "                                feature_names=iris_dataset.feature_names,\n",
    "                                class_names=iris_dataset.target_names,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "\n",
    "# create a .png file from the .dot file \n",
    "from subprocess import call\n",
    "call(['dot', '-T', 'png', 'iris_IG.dot', '-o', 'iris_IG.png']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to your working folder an **open the file `'iris_IG.png'`**. \n",
    "\n",
    "As you can see **the decision tree learned using the Information Gain is the following:**\n",
    "\n",
    "<img src=\"iris_IG.png\" alt=\"iris_IG.png\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Learning a Decision Tree using Train and Test Data\n",
    "\n",
    "Recall what we did at the end of Practical 02 + 03 with Iris Dataset. **Repeat the train and test scheme followed then by using now the decision tree as classifier instead of the K-NN used before.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's up to you to code\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Additional Exercise: Learn a Decision Tree using Train and Test Data for the Wine Dataset\n",
    "\n",
    "The file `wine.csv` constains the [Wine Dataset](https://archive.ics.uci.edu/ml/datasets/wine). These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \n",
    "\n",
    "**You should first load the dataset from a .csv file and can use the code below to do it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(fname):\n",
    "    \"\"\"Load CSV file with any number of consecutive features, starting in column 0, where last column is the class\"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    nc = df.shape[1] # number of columns\n",
    "    matrix = df.as_matrix() # Convert dataframe to darray\n",
    "    table_X = matrix [:, 0:nc-1] # get features \n",
    "    table_y = matrix [:, nc-1] # get class (last columns)           \n",
    "    features = df.columns.values[0:nc-1] #get features names\n",
    "    target = df.columns.values[nc-1] #get target name\n",
    "    return table_X, table_y, features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset `wine.csv`\n",
    "table_X, table_y, features, target = load_data('wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's up to you to code\n",
    "# train and test the decision tree\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Decision Trees from Categorical Attributes\n",
    "\n",
    "In the previous examples, the features were numeric (real-valued). In many settings, this is not the case, and some or even all features are categorical.  \n",
    "\n",
    "**Scikit-learn only handles numeric features, but provides mechanisms to convert categorical features into numeric ones**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Handling Categorical Features with Two Values\n",
    "\n",
    "**For this example, all features are categorical, each with two possible values.** \n",
    "\n",
    "**We will convert each feature into an integer value, with two possible values, either 0 or 1.**\n",
    "\n",
    "We consider the dataset `hike.csv`, where all features have the following values {\"yes\", \"no\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(fname):\n",
    "    \"\"\"Load CSV file with any number of consecutive features, starting in column 0, where last column is tha class\"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    nc = df.shape[1] # number of columns\n",
    "#    matrix = df.as_matrix() # Convert dataframe to darray # deprecating...\n",
    "    matrix = df.values # Convert dataframe to darray\n",
    "    table_X = matrix [:, 0:nc-1] # get features \n",
    "    table_y = matrix [:, nc-1] # get class (last columns)           \n",
    "    features = df.columns.values[0:nc-1] #get features names\n",
    "    target = df.columns.values[nc-1] #get target name\n",
    "    return table_X, table_y, features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_X, table_y, features, target = load_data('hike.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from which we should learn (features)\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first column 'Sample' should not be used to learn the decision tree since it is the identifier \n",
    "# let's remove it from table_X\n",
    "\n",
    "nc = table_X.shape[1] # number of columns\n",
    "table_X = table_X[:, 1:nc] # remove column 0\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also remove 'Sample' from the features names\n",
    "features = features [1:features.size]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target name\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector with what we should learn (Class)\n",
    "table_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we need to define utility functions to transform the features and the classe into binary values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def int_encode_class(vect):\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(vect)\n",
    "    integer_classes = label_encoder.transform(label_encoder.classes_)\n",
    "    t = label_encoder.transform(vect)\n",
    "    return t\n",
    "    \n",
    "def int_encode_feature(vect):\n",
    "    return int_encode_class(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the utility functions, we can now set up the scikit-learn classifier as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE table_X (FEATURES) with integers \n",
    "\n",
    "table_X[:, 0] = int_encode_feature(table_X[:, 0])\n",
    "table_X[:, 1] = int_encode_feature(table_X[:, 1])\n",
    "table_X[:, 2] = int_encode_feature(table_X[:, 2])\n",
    "table_X[:, 3] = int_encode_feature(table_X[:, 3])\n",
    "\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE table_Y (CLASS) with integers\n",
    "\n",
    "table_y = int_encode_class(table_y)\n",
    "\n",
    "table_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We still have to convert the binary values into real values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert FEATURES into REAL VALUES\n",
    "\n",
    "table_X = table_X.astype(float)\n",
    "\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvertCLASS to REAL VALUES\n",
    "\n",
    "table_y = table_y.astype(float)\n",
    "\n",
    "table_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now finally learn the **decision tree classifier** (http://scikit-learn.org/stable/modules/tree.html) using the class `DecisionTreeClassifier` (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). Remember that the parameter `criterion` defines the function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Gini is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#Learning a decision tree using CART and the Gini index as impurity criteria (default)\n",
    "\n",
    "dtc_Gini = tree.DecisionTreeClassifier() #criterion='gini'\n",
    "dtc_Gini = dtc_Gini.fit(table_X, table_y)\n",
    "\n",
    "dtc_Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we **create the .dot file via scikit-learn using the export_graphviz function from the tree submodule**, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a .dot file with the tree\n",
    "tree.export_graphviz(dtc_Gini, out_file='hike_Gini.dot',\n",
    "                     feature_names=['Lecture', 'Concert', 'ArtExpo', 'SeasonSales'],\n",
    "                     class_names=['No', 'Yes'],\n",
    "                     filled=True, rounded=True,\n",
    "                     special_characters=True)\n",
    "\n",
    "#after executing this code you should have the file \"hike_Gini.dot\" in your working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have installed GraphViz on our computer, we can **convert the tree.dot file into a PNG file** by executing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a .png file from the .dot file\n",
    "from subprocess import call\n",
    "call(['dot', '-T', 'png', 'hike_Gini.dot', '-o', 'hike_Gini.png'])\n",
    "\n",
    "#after executing this code you should have the file \"hike_Gini.png\" in your working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The decision tree learned using the Gini Index is the following:**\n",
    "\n",
    "<img src=\"hike_Gini.png\" alt=\"hike_Gini.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now check if we obtain the same decision tree if we used Information Gain instead of Gini Index:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#Learning a decision tree using CART and the Information Gain as impurity criteria (default)\n",
    "\n",
    "dtc_IG = tree.DecisionTreeClassifier(criterion = \"entropy\")\n",
    "dtc_IG = dtc_IG.fit(table_X, table_y)\n",
    "\n",
    "dtc_IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a .dot file with the tree\n",
    "tree.export_graphviz(dtc_IG, out_file='hike_IG.dot',\n",
    "                     feature_names=['Lecture', 'Concert', 'ArtExpo', 'SeasonSales'],\n",
    "                     class_names=['No', 'Yes'],\n",
    "                     filled=True, rounded=True,\n",
    "                     special_characters=True)\n",
    "\n",
    "# create a .png file from the .dot file\n",
    "from subprocess import call\n",
    "call(['dot', '-T', 'png', 'hike_IG.dot', '-o', 'hike_IG.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The decision tree learned using the Information Gain is the following:**\n",
    "\n",
    "<img src=\"hike_IG.png\" alt=\"hike_IG.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Exercise: Learn Decision Trees for the Zoo Dataset\n",
    "\n",
    "Do the necessary encodings to learn a decision tree for the Zoo dataset in file `zoo.csv`. \n",
    "\n",
    "You can also use the files  `zoo_train.csv` and `zoo_test.csv` as train and test sets as we did in Orange3. The first loading is already done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(fname):\n",
    "    \"\"\"Load CSV file with any number of consecutive features, starting in column 0, where last column is tha class\"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    nc = df.shape[1] # number of columns\n",
    "    matrix = df.as_matrix() # Convert dataframe to darray\n",
    "    table_X = matrix [:, 0:nc-1] # get features \n",
    "    table_y = matrix [:, nc-1] # get class (last columns)           \n",
    "    features = df.columns.values[0:nc-1] #get features names\n",
    "    target = df.columns.values[nc-1] #get target name\n",
    "    return table_X, table_y, features, target\n",
    "\n",
    "table_X, table_y, features, target = load_data('zoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is up to you to code\n",
    "#  ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Handling Categorical Features with Multiple Values\n",
    "\n",
    "**For categorical features with more than two possible values, a different approach is used. The idea is to encode each possible value as a distinct feature, using the so-called one-hot-encoding.**\n",
    "\n",
    "Let's see how this is done using the small dataset in `votingIssue.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(fname):\n",
    "    \"\"\"Load CSV file with any number of consecutive features, starting in column 0, where last column is tha class\"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    nc = df.shape[1] # number of columns\n",
    "    matrix = df.as_matrix() # Convert dataframe to darray\n",
    "    table_X = matrix [:, 0:nc-1] # get features \n",
    "    table_y = matrix [:, nc-1] # get class (last columns)           \n",
    "    features = df.columns.values[0:nc-1] #get features names\n",
    "    target = df.columns.values[nc-1] #get target name\n",
    "    return table_X, table_y, features, target\n",
    "\n",
    "table_X, table_y, features, target = load_data('votingIssue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first column 'Person' should not be used to learn the decision tree since it is the identifier \n",
    "# let's remove it from table_X\n",
    "nc = table_X.shape[1] # number of columns\n",
    "table_X = table_X[:, 1:nc] # remove column 0\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also remove 'Person' from the features names\n",
    "features = features [1:features.size]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We start by defining the function `ohenc_encode_feature` that given a target column (`col`), the number of rows (`nrow`) and the number of possible values (`ndim`), replaces the original column by `ndim` new binary colums.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def ohenc_encode_feature(table_X, col, nrow, ndim):\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(table_X[:, col])\n",
    "    integer_classes = label_encoder.transform(label_encoder.classes_).reshape(ndim, 1)\n",
    "    enc = OneHotEncoder()\n",
    "    one_hot_encoder = enc.fit(integer_classes)\n",
    "    # First, convert feature values to 0-(N-1) integers using label_encoder\n",
    "    num_of_rows = nrow\n",
    "    t = label_encoder.transform(table_X[:, col]).reshape(num_of_rows, 1)\n",
    "    # Second, create a sparse matrix with col columns, each one indicating\n",
    "    # whether the instance belongs to the class\n",
    "    new_features = one_hot_encoder.transform(t)\n",
    "    # Add the new features to table_X\n",
    "    table_X = np.concatenate([table_X, new_features.toarray()], axis = 1)\n",
    "    # Eliminate converted columns\n",
    "    table_X = np.delete(table_X, [col], 1)\n",
    "    return table_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We still need the functions `int_encode_feature` and `int_encode_class`, since we have two features with two values ('Sex' and 'HasChildren'), and need to encode the target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def int_encode_class(vect):\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(vect)\n",
    "    integer_classes = label_encoder.transform(label_encoder.classes_)\n",
    "    t = label_encoder.transform(vect)\n",
    "    return t\n",
    "    \n",
    "def int_encode_feature(vect):\n",
    "    return int_encode_class(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting all together, we can perform the encoding for the VotingIssue dataset as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode  feature 'Sex'\n",
    "\n",
    "table_X[:, 2] = int_encode_feature(table_X[:, 2])\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode  feature 'HasChildren'\n",
    "\n",
    "table_X[:, 3] = int_encode_feature(table_X[:, 3])\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode feature 'Education'\n",
    "\n",
    "# 1st - int_encode_feature\n",
    "table_X[:,0] = int_encode_feature(table_X[:, 0])\n",
    "\n",
    "# 2nd - ohenc_encode_feature\n",
    "num_of_rows = table_X.shape[0]\n",
    "table_X = ohenc_encode_feature(table_X, 0, num_of_rows, 3)\n",
    "\n",
    "# 3rd Update feature names\n",
    "features = ['MaritalStatus', 'Sex', 'HasChildren', 'Education-Primary', 'Education-Secondary', 'Education-University']\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode feature 'MaritalStatus'\n",
    "\n",
    "# 1st - int_encode_feature\n",
    "table_X[:,0] = int_encode_feature(table_X[:, 0])\n",
    "\n",
    "# 2nd - ohenc_encode_feature\n",
    "num_of_rows = table_X.shape[0]\n",
    "table_X = ohenc_encode_feature(table_X, 0, num_of_rows, 3)\n",
    "\n",
    "# 3rd Update feature names\n",
    "features = ['Sex', 'HasChildren', 'Education-Primary', 'Education-Secondary', \n",
    "            'Education-University', 'MaritalStatus-Single', 'MaritalStatus-Married', 'MaritalStatus-Divorced']\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert table_X to numerical values\n",
    "table_X = table_X.astype(float)\n",
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE table_y (CLASS) with integers\n",
    "table_y = int_encode_class(table_y)\n",
    "\n",
    "# Convert table_y to numerical values\n",
    "table_y = table_y.astype(float)\n",
    "table_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that the data is encoded we can learn the decision tree.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the decision tree\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "cdt = tree.DecisionTreeClassifier(criterion='gini')\n",
    "cdt = cdt.fit(table_X, table_y)\n",
    "cdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "\n",
    "tree.export_graphviz(cdt, out_file='votingIssue.dot',\n",
    "                     feature_names=features,\n",
    "                     filled=True, rounded=True,\n",
    "                     special_characters=True)\n",
    "\n",
    "from subprocess import call\n",
    "call(['dot', '-T', 'png', 'votingIssue.dot', '-o', 'votingIssue.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The decision tree learned using the Gini Index is the following:**\n",
    "\n",
    "<img src=\"votingIssue.png\" alt=\"votingIssue.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Decision Trees from Multiple Types of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. A Small Dataset: The Restaurant Dataset\n",
    "\n",
    "Do the necessary encodings to learn a decision tree for the Restaurant dataset in file `restaurant.csv`. The loading is already done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(fname):\n",
    "    \"\"\"Load CSV file with any number of consecutive features, starting in column 0, where last column is tha class\"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    nc = df.shape[1] # number of columns\n",
    "    matrix = df.as_matrix() # Convert dataframe to darray\n",
    "    table_X = matrix [:, 0:nc-1] # get features \n",
    "    table_y = matrix [:, nc-1] # get class (last columns)           \n",
    "    features = df.columns.values[0:nc-1] #get features names\n",
    "    target = df.columns.values[nc-1] #get target name\n",
    "    return table_X, table_y, features, target\n",
    "\n",
    "table_X, table_y, features, target = load_data('restaurante.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is up to you to code\n",
    "#  ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. A Not That Small Dataset: The Titanic Dataset\n",
    "\n",
    "Do the necessary data preprocessing to learn a decision tree for the Titanic dataset in file `titanic.csv`. The loading is already done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(fname):\n",
    "    \"\"\"Load CSV file with any number of consecutive features, starting in column 0, where last column is tha class\"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    nc = df.shape[1] # number of columns\n",
    "    matrix = df.as_matrix() # Convert dataframe to darray\n",
    "    table_X = matrix [:, 0:nc-1] # get features \n",
    "    table_y = matrix [:, nc-1] # get class (last columns)           \n",
    "    features = df.columns.values[0:nc-1] #get features names\n",
    "    target = df.columns.values[nc-1] #get target name\n",
    "    return table_X, table_y, features, target\n",
    "\n",
    "table_X, table_y, features, target = load_data('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is up to you to code\n",
    "#  ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
